{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI gym - MountainCar\n",
    "\n",
    "### Open AI gym installeerimine\n",
    "\n",
    "Piisab ka `pip3 install gym`, kuid sedasi ei saa kõiki erinevaid keskkondi alla laadida. Kõikide keskkondade installeerimiseks sobib:\n",
    "\n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip3 install -e .\n",
    "pip3 install -e '.[all]'\n",
    "```\n",
    "\n",
    "Rohkem infot: http://gym.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install gym tensorflow numpy keyboard matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainCar\n",
    "\n",
    "Alustame lihtsamast - http://gym.openai.com/envs/MountainCar-v0/\n",
    "\n",
    "> A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "\n",
    "- observation space (2) - position [-1.2, 0.6], velocity [-0.07, 0.07] (suunaline)\n",
    "- eesmärk - position 0.5\n",
    "- action space (3) - parem, vasak, *ära tee midagi*\n",
    "\n",
    "### Teeme esmalt käsitsi juhitava klassi\n",
    "\n",
    "Keyboard nõuab administraatori õigusi. `sudo` aitab: `sudo jupyter notebook --allow-root`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyboard import is_pressed\n",
    "\n",
    "class HumanAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.actions = {\n",
    "            'left': 0,\n",
    "            'right': 2,\n",
    "            'neutral': 1\n",
    "        }\n",
    "        \n",
    "    def act(self, _):\n",
    "        if is_pressed('left'):\n",
    "            act = 'left'\n",
    "        elif is_pressed('right'):\n",
    "            act = 'right'\n",
    "        else:\n",
    "            act = 'neutral'\n",
    "\n",
    "        return self.actions[act]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = HumanAgent()\n",
    "\n",
    "for i in range(10):\n",
    "    print('Episode', i, end='\\r')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kuidas panna arvuti õppima?\n",
    "\n",
    "Reeglina tähendab 'õppimine' masinõppe keeles mingi funktsiooni minimeerimist (vahel ka maksimeerimist). See tähendab, et esmalt tuleb probleem defineerida funktsioonina $y = f(X)$, kus y on soovitud tulemus, ning X sisendandmed. Masinõppes on arvuti ülesandeks leida $f(x)$, mille $y - \\hat{y}$ on võimalikult väike. Teisisõnu püüab arvuti õppimise käigus minimeerida *õigete* tulemuste ja *ennustatud* tulemuste vahet.\n",
    "\n",
    "Olenevalt ülesandest võib y juba eelnevalt olemas olla (juhendatud õpe; [*supervised learning*](https://en.wikipedia.org/wiki/Supervised_learning)), see võib puududa üleüldse (juhendamata õpe; [*unsupervised learning*](https://en.wikipedia.org/wiki/Unsupervised_learning)) või andmeid võidakse koguda käigult saades oma keskkonnalt tagasisidet (stiimulõpe; [*reinforcement learning*](https://en.wikipedia.org/wiki/Reinforcement_learning)). Open AI gym keskendub neist viimasele, pakkudes omalt poolt mitmeid keskkondi, mille abil andmeteadlased saavad õppida ja uusi masinõppe mudeleid leiutada.\n",
    "\n",
    "Stiimulõppe arhitektuur koosneb lihtsustatult vaadeldavast keskkonnast (X-id), agendist (mudel), agendi tehtud otsustest ($\\hat{y}$) ning saadud tagasisidest (y). Tagasisideks (*reward*) on langetatud otsuse väärtus ja seega on agendi ülesanne on õppida langetama otsuseid, mis maksimeerivad saadud tagasisidet.\n",
    "\n",
    "![Reinforcement learning](Reinforcement_learning_diagram.svg)\n",
    "\n",
    "Antud juhul on X-ideks `observation` (käru positsioon ja kiirus) ja soovitud tulemus `done` väärtus (positsioon == 0.5). Praktilises töös kulub valdav osa andmeteadlaste ajast X-ide ja y-te defineerimisele, sest mida selgem on seos sisendite ja väljundite vahel, seda lihtsam on arvutil õppida nende vaheline seos. Antud juhul on probleem  raskesti õpitav, sest eesmärk on diskreetne (*True / False*) ning enamuse ajast mitteinformatiivne. See, et suurema osa ajast on eesmärk saavutamata ei ütle midagi selle kohta, kui lähedal eesmärgi saavutamisele ollakse. Püüame seda muuta.\n",
    "\n",
    "Ütleme, et eesmärgiks on hoopis maksimeerida liikumise kiirust, sest teame, et eesmärgi saavutamine on kiirusega korrelatsioonis ning kiirus annab meile pidevat tagasisidet, kui kaugel on eesmärgi saavutamine. Teiseks püüame panna arvutit õppima, kui suur on kiirus pärast ühe või teise valiku tegemist (*parem, vasak, ära tee midagi*), ehk mis on erinevate valikute väärtus tulevikus. Seda nimetatakse [q-learninguks](https://en.wikipedia.org/wiki/Q-learning) (q ehk *quality*). Täpsemalt näeb see valemi (*Bellmani võrrand*) kujul välja järgnev:\n",
    "\n",
    "\n",
    "$$Q^{new}(s, a) = Q(s, a) + \\alpha[R(s, a) + \\gamma \\max Q'(s', a') - Q(s, a)]$$\n",
    "\n",
    "kus:\n",
    "- Q - q-väärtus\n",
    "- s - hetkeseisund (*state*)\n",
    "- a - tehtud valik (*action*)\n",
    "- $\\alpha$ - õpisamm\n",
    "- R - saadud kasu (*reward*)\n",
    "- $\\gamma$ - tuleviku realiseerumise tõenäosus\n",
    "- $\\max Q'(s', a') - Q(s, a)$ ennustatud maksimaalne tulevikus saadav lisandväärtus\n",
    "\n",
    "Seame ülesse õppimisalgoritmi ja paneme q-valemi koodi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "tfk = tensorflow.keras\n",
    "\n",
    "class AIAgent:\n",
    "    \"\"\"\n",
    "    A simple neural network based AI agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, train, model_weights_dir=None):\n",
    "        self.train = train\n",
    "        self.memory = deque(maxlen=200)\n",
    "        self.training_losses = deque(maxlen=10000)\n",
    "        self.previous_state = None\n",
    "        self.model_weights_dir = model_weights_dir\n",
    "        self.model = self.create_model()\n",
    "        \n",
    "        self.alpha = 0.1\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "    def act(self, obs):\n",
    "        \"\"\"\n",
    "        Outputs model actions given the state.\n",
    "        \"\"\"\n",
    "        obs = self.reshape_single_obs(obs)\n",
    "        pred = self.model.predict(obs)\n",
    "        return np.argmax(pred)\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Defines the AI model.\n",
    "        \"\"\"\n",
    "        model = tfk.Sequential([\n",
    "            tfk.layers.Input(shape=(2)),\n",
    "            tfk.layers.Dense(32, activation='relu'),\n",
    "            tfk.layers.Dense(3, activation='linear')\n",
    "        ], name='neural_net')\n",
    "        \n",
    "        model.compile(loss='mse', optimizer=tfk.optimizers.Adam(lr=1e-3))\n",
    "        \n",
    "        if self.model_weights_dir:\n",
    "            try:\n",
    "                model.load_weights(self.model_weights_dir)\n",
    "                print('Pretrained weights initialized.')\n",
    "            except:\n",
    "                print('New weights initialized.')\n",
    "            \n",
    "        return model\n",
    "        \n",
    "    def save_memory(self, obs, reward, action, done):\n",
    "        \"\"\"\n",
    "        Save episodes and Q values for training the model.\n",
    "        \"\"\"\n",
    "        obs = self.reshape_single_obs(obs)\n",
    "        \n",
    "        # calculate new Q value if previous state exists\n",
    "        if self.previous_state:\n",
    "            current_q = self.previous_state['reward']\n",
    "            predicted_q = np.max(self.model.predict(obs))\n",
    "            \n",
    "            new_q = current_q + self.alpha * (reward + self.gamma * predicted_q - current_q)\n",
    "            targets = np.zeros((1,3))\n",
    "            targets[0][action] = new_q\n",
    "            \n",
    "            self.memory.append((obs, targets))\n",
    "        \n",
    "        # train model on batch if memory is populated\n",
    "        if self.train and len(self.memory) == self.memory.maxlen:\n",
    "            self.train_model()\n",
    "        \n",
    "        # reset prev state if episode is completed\n",
    "        if done:\n",
    "            self.previous_state = None\n",
    "        else:\n",
    "            self.previous_state = {\n",
    "                'obs': obs,\n",
    "                'reward': reward,\n",
    "                'action': action\n",
    "            }\n",
    "            \n",
    "    def train_model(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        Trains the model on minibatch.\n",
    "        \"\"\"\n",
    "        # sample random episides from memory\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        X = []\n",
    "        y = []\n",
    "        for row in batch:\n",
    "            X.append(row[0])\n",
    "            y.append(row[1])\n",
    "        X = np.vstack(X)\n",
    "        y = np.vstack(y)\n",
    "        loss = self.model.train_on_batch(X, y)\n",
    "        self.training_losses.append(loss)\n",
    "        if self.model_weights_dir:\n",
    "            self.model.save(self.model_weights_dir)\n",
    "            \n",
    "    def reshape_single_obs(self,obs):\n",
    "        \"\"\"\n",
    "        Reshapes a single observation to fit the model.\n",
    "        \"\"\"\n",
    "        return obs.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights initialized.\n"
     ]
    }
   ],
   "source": [
    "agent = AIAgent(train=True, model_weights_dir='model_checkpoints/dqn.h5')\n",
    "\n",
    "for i in range(50):\n",
    "    print('Episode', i + 1, 'Mean reward ...', end='\\r')\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.act(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        reward = abs(observation[1])**2\n",
    "        agent.save_memory(observation, reward, action, done)\n",
    "        rewards.append(reward)\n",
    "    print('Episode', i + 1, 'Mean reward', round(np.mean(rewards),4))\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MountainCar](mountain_car.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline \n",
    "#%config InlineBackend.figure_format = 'png'\n",
    "#plt.style.use('ggplot')\n",
    "#\n",
    "#plt.figure(figsize=(16,4))\n",
    "#labels = ['total', 'left', 'neutral', 'right']\n",
    "#for i in range(4):\n",
    "#    plt.plot(np.vstack(agent.training_losses)[:,i], label=labels[i])\n",
    "#plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=4)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
